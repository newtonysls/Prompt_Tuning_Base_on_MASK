# Prompt_Tuning_Base_on_MASK
In our opinion, during the processing of pre-training, the special token "[MASK]" has learned much more informations than others for predicting the correct word undering the MASK. In this case, in our work, we make giant efforts on this issue and research the impact of "[MASK]" to language models. As result, we find prompt tuning based on MASK is faster and better than finetune and the current prompt tuning methods. Besides, we also use the method we proposed in many down-stream tasks to explore the performance in diffierent scene. Finally, our method archive SOTA results to prove the effectives.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64780ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# 放在第一句，不然会报错\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "from operator import index\n",
    "import os\n",
    "import random\n",
    "from xml.dom.minidom import Document\n",
    "from matplotlib.pyplot import title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, Dataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from tqdm import tqdm, trange\n",
    "from random import random, randrange, randint, shuffle, choice, sample\n",
    "from transformers import BertModel,BertConfig,BertTokenizer,BertForMaskedLM\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import (\n",
    "    get_constant_schedule,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_polynomial_decay_schedule_with_warmup,\n",
    ")\n",
    "from data_processor import RTE_Processing,CB_Processing,BoolQ_Processing,Json_File_Reader\n",
    "import protum_args as args\n",
    "import jieba\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics  import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8638ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtumDataset(Dataset):\n",
    "    def __init__(self,data,max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        data_item = self.data[index]\n",
    "        input_ids = data_item['input_ids']\n",
    "        segment_ids = data_item['segment_ids']\n",
    "        attention_ids = data_item['attention_ids']\n",
    "        prompt_positions = data_item['prompt_positions']\n",
    "        label_ids = data_item['label_ids']\n",
    "\n",
    "        assert len(input_ids)==len(segment_ids)==len(attention_ids)\n",
    "        padding_length = self.max_seq_length - len(input_ids)\n",
    "        input_ids += [0] * padding_length\n",
    "        attention_ids += [0] * padding_length\n",
    "        segment_ids += [0] * padding_length\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        segment_ids = torch.tensor(segment_ids,dtype=torch.long)\n",
    "        attention_ids = torch.tensor(attention_ids,dtype=torch.long)\n",
    "        prompt_positions = torch.tensor(prompt_positions,dtype=torch.long)\n",
    "        label_ids = torch.tensor(label_ids,dtype=torch.long)\n",
    "        return input_ids, segment_ids, attention_ids, prompt_positions,label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80755975",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args.model_name_from_hugging_face)\n",
    "\n",
    "vocab_list = list(tokenizer.vocab.keys())\n",
    "config = BertConfig.from_pretrained(args.model_name_from_hugging_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c58ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Json_File_Reader('RTE',args.train_data_path)\n",
    "eval_data = Json_File_Reader('RTE',args.dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "065129a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RTE Data Processing: 277it [00:00, 592.61it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_processor = RTE_Processing(eval_data,\n",
    "                 tokenizer,\n",
    "                 args.max_seq_length,\n",
    "                 vocab_list,\n",
    "                 is_pretraining=False)\n",
    "eval_inputs = eval_processor.Creat_Input_For_PLMs()\n",
    "EvalDataset = ProtumDataset(eval_inputs,args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934c3c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b188bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  6355,   117,  1195,  1208,  1132, 15137,  1115,  2848, 25523,\n",
       "          1132,  3196,  1147, 12949,  1222,  6946,   119, 20012,   118,  3989,\n",
       "         10548,  1132,   182, 15012,  1916,  4946,  1190,  1195,  1169,  1435,\n",
       "          1146,  1114,  1207,  2848, 25523,  1106,  2147,  1103,  1207,  9138,\n",
       "           119, 22171,   131, 18757, 25857,  1465,  1110,  2183,  1103,  1594,\n",
       "          1222,  2848, 25523,   136,  1109, 26018,   131,   103,   119,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([57]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvalDataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e3a44ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['[CLS]',\n",
       "  'Yet',\n",
       "  ',',\n",
       "  'we',\n",
       "  'now',\n",
       "  'are',\n",
       "  'discovering',\n",
       "  'that',\n",
       "  'anti',\n",
       "  '##biotics',\n",
       "  'are',\n",
       "  'losing',\n",
       "  'their',\n",
       "  'effectiveness',\n",
       "  'against',\n",
       "  'illness',\n",
       "  '.',\n",
       "  'Disease',\n",
       "  '-',\n",
       "  'causing',\n",
       "  'bacteria',\n",
       "  'are',\n",
       "  'm',\n",
       "  '##uta',\n",
       "  '##ting',\n",
       "  'faster',\n",
       "  'than',\n",
       "  'we',\n",
       "  'can',\n",
       "  'come',\n",
       "  'up',\n",
       "  'with',\n",
       "  'new',\n",
       "  'anti',\n",
       "  '##biotics',\n",
       "  'to',\n",
       "  'fight',\n",
       "  'the',\n",
       "  'new',\n",
       "  'variations',\n",
       "  '.',\n",
       "  'Question',\n",
       "  ':',\n",
       "  'Ba',\n",
       "  '##cter',\n",
       "  '##ia',\n",
       "  'is',\n",
       "  'winning',\n",
       "  'the',\n",
       "  'war',\n",
       "  'against',\n",
       "  'anti',\n",
       "  '##biotics',\n",
       "  '?',\n",
       "  'The',\n",
       "  'Answer',\n",
       "  ':',\n",
       "  '[MASK]',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " 'input_ids': [101,\n",
       "  6355,\n",
       "  117,\n",
       "  1195,\n",
       "  1208,\n",
       "  1132,\n",
       "  15137,\n",
       "  1115,\n",
       "  2848,\n",
       "  25523,\n",
       "  1132,\n",
       "  3196,\n",
       "  1147,\n",
       "  12949,\n",
       "  1222,\n",
       "  6946,\n",
       "  119,\n",
       "  20012,\n",
       "  118,\n",
       "  3989,\n",
       "  10548,\n",
       "  1132,\n",
       "  182,\n",
       "  15012,\n",
       "  1916,\n",
       "  4946,\n",
       "  1190,\n",
       "  1195,\n",
       "  1169,\n",
       "  1435,\n",
       "  1146,\n",
       "  1114,\n",
       "  1207,\n",
       "  2848,\n",
       "  25523,\n",
       "  1106,\n",
       "  2147,\n",
       "  1103,\n",
       "  1207,\n",
       "  9138,\n",
       "  119,\n",
       "  22171,\n",
       "  131,\n",
       "  18757,\n",
       "  25857,\n",
       "  1465,\n",
       "  1110,\n",
       "  2183,\n",
       "  1103,\n",
       "  1594,\n",
       "  1222,\n",
       "  2848,\n",
       "  25523,\n",
       "  136,\n",
       "  1109,\n",
       "  26018,\n",
       "  131,\n",
       "  103,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'segment_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_ids': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'prompt_positions': [57],\n",
       " 'label_ids': 0,\n",
       " 'data_name': 'RTE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ce8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
